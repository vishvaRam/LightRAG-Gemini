# ğŸ“š LightRAG + Gemini (Dockerized RAG)

A lightweight **Retrieval-Augmented Generation (RAG)** project using **LightRAG (HKU)** and **Google Gemini** models.  
It ingests large text files, generates embeddings, and answers questions using multiple retrieval strategies.

---

## âœ¨ Features

- ğŸ” Retrieval modes: **Naive, Local, Global, Hybrid**
- ğŸ¤– LLM: `gemini-2.0-flash`
- ğŸ§  Embeddings: `text-embedding-004`
- âš¡ Async-first design
- ğŸ“¦ Docker support
- ğŸ’¾ Persistent vector storage

---

## ğŸ“ Project Structure

```

app.py # Async RAG pipeline
demo.py # Minimal demo
Dockerfile
requirements.txt
rag_storage/ # Auto-created storage

```

---

## ğŸ”‘ Setup

```bash
export GEMINI_API_KEY="your_api_key_here"
pip install -r requirements.txt
```

---

## â–¶ï¸ Run

### Full Async App

```bash
python app.py
```
---

## ğŸ§  Models Used

| Purpose   | Model              |
| --------- | ------------------ |
| LLM       | gemini-2.0-flash   |
| Embedding | text-embedding-004 |

---

## ğŸ“œ License

MIT

---

â­ Star the repo if you find it useful!

